{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implementation of Neural Network\n",
    "\n",
    "[online_link](https://github.com/har33sh/ArtificialNeuralNetwork)\n",
    "\n",
    "# Neural Network\n",
    "- Implemenation of Neural Network\n",
    "- Done in Python, Pandas and Numpy with no external machine learning used\n",
    "<br />\n",
    "The purpose of this project was to understand the architecture of Neural Network and to know what is going on during the training process.<br />\n",
    "\n",
    "**This is not a production quality code**\n",
    "<br />\n",
    "\n",
    "## About the Implemenation\n",
    " Implemenation is inspired by the [MLPClassifier of sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)  \n",
    "\n",
    "### Configurable Parameters\n",
    "- **hidden_layer_size**:\n",
    "    The number of hidden layersm by default the size of hidden layer is set to be 3, as in most of the cases 3 layers is good.\n",
    "- **learning_rate**: The rate at which the  weights are updated\n",
    "- **neurons**: The number of neurons in the hidden layers\n",
    "- **activation_function**\n",
    "  - tanh: the hyperbolic tan function, returns f(x) = tanh(x). *This is the default*\n",
    "  - relu: the rectified linear unit function, returns f(x) = max(0, x)\n",
    "  - sigmoid: the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "- **iterations**: Maximum number of iterations.\n",
    "- **decay_factor**:Should be between 0 and 1. The rate at which the learning_rate rate is decayed\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "(https://inclass.kaggle.com/c/cs-725-403-assignment-2)\n",
    "\n",
    "Task is to predict whether income exceeds 50K/yr based on census data. Also known as \" Census Income \" dataset.\n",
    "<br />\n",
    "Note that in the train and test data,salary >50K is represented by 1 and <=50K is represented by 0.\n",
    "<br />\n",
    "\n",
    "To know more about the dataset [click here](https://har33sh.github.io/ArtificialNeuralNetwork/)\n",
    "<br />\n",
    "\n",
    "\n",
    "## References\n",
    "1. [Census Income Data Set from archive.ics.uci.edu ](https://archive.ics.uci.edu/ml/datasets/Census+Income)\n",
    "2. [A guide to Deep learning](http://yerevann.com/a-guide-to-deep-learning/)\n",
    "3. [Information on how to optimise Neural Network](http://cs231n.github.io/neural-networks-3/)\n",
    "4. [Neural Network in 11 lines -- Short and best Implemenation of NN ](http://iamtrask.github.io/2015/07/12/basic-python-network/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "class neural_network:\n",
    "    def __init__(self,hidden_layer_size=3,learning_rate=0.01,neurons=20,iterations=60000,activation_function='tan',decay_factor=0.95):\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.activation_function=activation_function\n",
    "        self.learning_rate=learning_rate\n",
    "        self.layer=list()\n",
    "        self.layer_weights=list()\n",
    "        self.output_layer=1\n",
    "        self.iterations=iterations\n",
    "        self.neurons=neurons\n",
    "        self.decay_factor=decay_factor\n",
    "        \n",
    "        \n",
    "    def create_network(self,X):\n",
    "        np.random.seed(1) #to have random in between the specific range\n",
    "        random_weights=2*np.random.random((X.shape[1],self.neurons))-1\n",
    "        self.layer_weights.append(random_weights)\n",
    "        for i in range(self.hidden_layer_size-2):\n",
    "            random_weights=2*np.random.random((self.neurons,self.neurons))-1\n",
    "            self.layer_weights.append(random_weights)\n",
    "        random_weights=2*np.random.random((self.neurons,self.output_layer))-1\n",
    "        self.layer_weights.append(random_weights)\n",
    "        \n",
    "        \n",
    "    def activation(self,x,derivative=False):\n",
    "        if derivative:\n",
    "            if self.activation_function == \"sigmoid\":\n",
    "                return x * (1 - x)\n",
    "            if self.activation_function==\"tan\":\n",
    "                return 1.0 - np.tanh(x)**2\n",
    "            if self.activation_function == \"ReLU\":\n",
    "                return (x > 0).astype(int)        \n",
    "        else:\n",
    "            if self.activation_function == \"sigmoid\":\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            if self.activation_function==\"tan\":\n",
    "                    return np.tanh(x)\n",
    "            if self.activation_function == \"ReLU\":\n",
    "                return x * (x > 0)\n",
    "            \n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        end_error=0\n",
    "        self.create_network(X)\n",
    "        for _ in range(self.iterations):\n",
    "            #feed forward throught the network\n",
    "            self.layer=list()\n",
    "            self.layer.append(X)\n",
    "            for i in range(self.hidden_layer_size):\n",
    "                hidden_layer=self.activation(np.dot(self.layer[i],self.layer_weights[i]))\n",
    "                self.layer.append(hidden_layer)\n",
    "            \n",
    "            error=Y-self.layer[-1]\n",
    "            end_error=np.mean(np.abs(error))\n",
    "#             if(_%100==1):\n",
    "#                 print(str(_)+\" Error \"+str(end_error))\n",
    "            for i in range(self.hidden_layer_size,0,-1):\n",
    "                delta = error*self.activation(self.layer[i],derivative=True)\n",
    "                error = delta.dot(self.layer_weights[i-1].T)\n",
    "                self.layer_weights[i-1] += self.learning_rate * (self.layer[i-1].T.dot(delta))\n",
    "            \n",
    "            self.learning_rate=self.learning_rate*self.decay_factor\n",
    "\n",
    "    \n",
    "    def predict(self,X):\n",
    "        predicted=X\n",
    "        for i in range(self.hidden_layer_size):\n",
    "            predicted=self.activation(np.dot(predicted,self.layer_weights[i]))\n",
    "        predict=predicted\n",
    "        if (self.activation_function=='sigmoid'):\n",
    "            predict[predict>0.5]=1\n",
    "            predict[predict<=0.5]=0\n",
    "        if(self.activation_function=='tan'):\n",
    "            predict[predict>0]=1\n",
    "            predict[predict<=0]=0\n",
    "        return predict.ravel()\n",
    "    \n",
    "    def score(self,X_test,Y_true):\n",
    "        predict=self.predict(X_test)\n",
    "        return np.sum(predict.ravel()==Y_true.ravel())/Y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data files and processing them\n",
    "\n",
    "Refer to Report of data visualizations and feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(inputData):\n",
    "    return (inputData - inputData.min()) / (inputData.max() - inputData.min())\n",
    "\n",
    "#reads the datafiles and returns the training and the testing data\n",
    "def get_data():\n",
    "    # get test & test csv files as a DataFrame\n",
    "    train_df = pd.read_csv(\"data/train.csv\")\n",
    "    test_df    = pd.read_csv(\"data/test.csv\")\n",
    "    \n",
    "    #feature engineering and removing features after analysis\n",
    "    cols_to_drop=['race','native-country','fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    for col in cols_to_drop:\n",
    "        train_df=train_df.drop([col],axis=1)\n",
    "        test_df=test_df.drop([col],axis=1)\n",
    "\n",
    "    numericalColumns = ('age', 'education-num')\n",
    "    for i in numericalColumns:\n",
    "        train_df[i] = normalize(train_df[i])\n",
    "        test_df[i] = normalize(test_df[i])\n",
    "\n",
    "    \n",
    "    #creating dummies of the data\n",
    "    train_df=pd.get_dummies(train_df)\n",
    "    test_df=pd.get_dummies(test_df)\n",
    "\n",
    "    #remove unwanted columns and the columns that are created for ?\n",
    "#     columns_to_remove=set(list(train_df)).symmetric_difference(set(list(test_df)))\n",
    "#     columns_to_remove.remove('salary')\n",
    "#     for col in list(train_df):\n",
    "#         if (col in columns_to_remove) or (\"?\" in col) :\n",
    "#             train_df=train_df.drop(col,1)\n",
    "#     for col in list(test_df):\n",
    "#         if (col in columns_to_remove) or (\"?\" in col) :\n",
    "#             test_df=test_df.drop(col,1)\n",
    "    \n",
    "    return train_df,test_df\n",
    "\n",
    "\n",
    "def process_data(percent):\n",
    "    train_df,test_df=get_data()\n",
    "    test_ids=test_df['id'].as_matrix()\n",
    "    train_df=train_df.drop(['id'],1)\n",
    "    test_df=test_df.drop(['id'],1)\n",
    "    train_df['const']=1\n",
    "    test_df['const']=1\n",
    "    Y=train_df['salary'].as_matrix()\n",
    "    X=train_df.drop(['salary'], axis=1).as_matrix()\n",
    "    Y=Y.reshape(len(Y),1)\n",
    "    end=int(X.shape[0] * percent)\n",
    "    #training data\n",
    "    train_X=X[:end,:]\n",
    "    train_Y=Y[:end,:]\n",
    "    #data for cross validation\n",
    "    cross_X=X[end:,:]\n",
    "    cross_Y=Y[end:,:]\n",
    "    #testing data\n",
    "    test_X=test_df.as_matrix()\n",
    "    return train_X,train_Y,cross_X,cross_Y,test_X,test_ids\n",
    "\n",
    "\n",
    "\n",
    "#writes the predicted values to file \n",
    "def write_result(ids,predicted,file_name):\n",
    "    output=np.column_stack((ids,predicted))\n",
    "    np.savetxt(file_name,output,delimiter=\",\",fmt=\"%d,%d\",header=\"id,salary\",comments ='')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Performing cross validation, Used 80% of the data for training and the remaining 20 for cross validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score  0.250416933932\n"
     ]
    }
   ],
   "source": [
    "#perform cross validation \n",
    "train_X,train_Y,cross_X,cross_Y,test_X,test_ids= process_data(0.80)\n",
    "nn=neural_network(hidden_layer_size=3,neurons=20,iterations=10000,learning_rate=0.01,activation_function='tan')\n",
    "nn.fit(train_X,train_Y)\n",
    "predict=nn.predict(cross_X)\n",
    "print(\"Score \",nn.score(cross_X,cross_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict=nn.predict(test_X)\n",
    "write_result(test_ids,predict,\"hareesh.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    \n",
    "## Classification using libraries and Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network : 0.715092112691\n",
      "Logistic Regression : 0.832555036691\n",
      "Support Vector Machines : 0.826089187664\n",
      "Random Forests : 0.80402319495\n",
      "K NN Classification : 0.825062862421\n",
      "Gaussian Naive Bayes : 0.623646533586\n"
     ]
    }
   ],
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "train_X,train_Y,cross_X,cross_Y,test_X,test_ids= process_data(0.50)\n",
    "X_train=train_X\n",
    "Y_train=train_Y.ravel()\n",
    "X_test=cross_X\n",
    "Y_test=cross_Y.ravel()\n",
    "\n",
    "#-----------  Neural Netowrk------------------\n",
    "nn=neural_network(hidden_layer_size=3,neurons=20,iterations=100,learning_rate=0.01,activation_function='tan')\n",
    "nn.fit(train_X,train_Y)\n",
    "print(\"Implemented Neural Network : \"+str(nn.score(cross_X, cross_Y)))\n",
    "\n",
    "\n",
    "#----------- Logistic Regression------------------\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "print(\"Logistic Regression : \"+ str(logreg.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#----------- Support Vector Machines------------------\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "print(\"Support Vector Machines : \"+str(svc.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#-----------  Random Forests------------------\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "print(\"Random Forests : \"+str(random_forest.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#----------- K NN Classification------------------\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "#-----------  Gaussian Naive Bayes------------------\n",
    "# gaussian = GaussianNB()\n",
    "# gaussian.fit(X_train, Y_train)\n",
    "# print(\"Gaussian Naive Bayes : \"+str(gaussian.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Implemented Neural Network vs  3 standard techniques \n",
    "<br />\n",
    "Above is the results obtained from the Neural Network implemented from scratch and the models that are implemented in scikit-learn\n",
    "<br/>\n",
    "Since the data can be lineary sepratable and are high dimensional spaces, the following models are choosen.<br/><br/>\n",
    "**Neural Network** : To compare between other ml algorithms<br/>\n",
    "**Logistic Regression** : Logistic regression models the probability of the default class.It's simple and fast, and the problem is not complex.<br/>\n",
    "**Support Vector Machines** :  Since the data can be lineary sepratable and SVM's are ffective in high dimensional spaces.<br/>\n",
    "**Random Forests** : A random forest fits a number of decision tree classifiers on sub-samples of the dataset and uses averaging to improve the predictive accuracy and to control over-fitting.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Error 0.297777800194\n",
      "End Error0.0817159495915\n",
      "[ 0.13131335  0.89094717  0.93470715  0.98301249]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#playgorund\n",
    "X= np.array([ [0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,10],\n",
    "              [10,1,1], ])\n",
    "\n",
    "Y=np.array([ [ 0,1,1,1 ] ]).T\n",
    "\n",
    "n=nn(hidden_layer_size=3,neurons=20,iterations=100,activation_function='sigmoid')\n",
    "n.fit(X,Y)\n",
    "predict=n.predict(X)\n",
    "print(predict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
